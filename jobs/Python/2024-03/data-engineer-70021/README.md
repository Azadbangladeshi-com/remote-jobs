# [Data Engineer](https://www.remotewlb.com/apply/data-engineer-70021)  
### Placer.ai  
#### `ðŸ’° ~0k` `ðŸŒŽ LATAM,Remote`  

Description

**ABOUT PLACER.AI:**

Placer.ai is a high-growth, big data pre-IPO tech company led by seasoned executives and repeat entrepreneurs who are building the world's first "Google Analytics for the physical world." Our platform provides instant visibility into any property in the U.S., presenting accurate details about visitation patterns and demographic breakdowns of visitors. Placer.aiâ€™s customers can see where visitors have been before, where they go afterwards, where they typically go for sports, entertainment, groceries, etc., and what their interests are. Placer.ai's A.I.-based SaaS platform replaces archaic solutions such as manual surveys, installed cameras, and other people-counting systems, creating a blue ocean market of more than $100B.

Placer.ai has grown 2x year-over-year for the past 2 years, counting more than 3,600+ paying customers across a range of industries, including 2 of the worldâ€™s top-10 retailers, 2 of the top-10 CPG firms worldwide, a worldâ€™s top hospitality firm, 2 of the worldâ€™s top-10 commercial real estate (CRE) firms and 2 of the worldâ€™s top multinational asset managers and hedge funds. We have raised $100M for Unicorn ($1B+) valuation in Series C funding.

**Summary** **:**

As a Data Engineer at Placer.ai, you play a pivotal role within our dynamic team by developing, optimizing, and managing our comprehensive data architecture. Your key responsibilities include designing scalable data pipelines for effective data integration and processing, collaborating with Data Scientists to support machine learning model training, and ensuring the integrity of data across our analytics platform. Your efforts foster cross-functional collaboration, guaranteeing that our data solutions meet business requirements and drive informed decision-making processes.

## Responsibilities:

  * Data Pipeline Development: Design, build, and optimize robust and scalable data pipelines to process, transform, and integrate large volumes of data from various sources into our analytics platform.
  * Work with Data Scientists to train machine learning models, track experiments and fine-tune models while maintaining cost efficiency
  * Data Quality Assurance: Implement data validation, cleansing, and enrichment techniques to ensure high-quality and consistent data across the platform.
  * Performance Optimization: Identify performance bottlenecks and optimize data processing and storage mechanisms to enhance overall system performance and reduce latency.
  * Cloud Infrastructure: Work extensively with cloud-based technologies (GCP), to design and manage scalable data infrastructure.
  * Collaboration: Collaborate with cross-functional teams including Data Analysts, Data Scientists, GTM Teams and Software Engineers to understand requirements and deliver solutions that meet business needs.
  * Monitoring and Maintenance: Monitor the health and performance of data pipelines, troubleshoot issues, and ensure high availability of data infrastructure

## Requirements:

  * Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
  * 5+ years of professional experience in software development, with at least 3 years as a Data Engineer.
  * Spark expertise (mandatory): Strong proficiency in Apache Spark, including hands-on experience with building data processing applications and pipelines using Spark's core libraries.
  * PySpark/Scala (mandatory): Proficiency in either PySpark (Python API for Spark) or Scala for Spark development.
  * Data Engineering: Proven track record in designing and implementing ETL pipelines, data integration, and data transformation processes.
  * Cloud Platforms: Hands-on experience with cloud platforms such as AWS, GCP, or Azure.
  * SQL and Data Modeling: Solid understanding of SQL, and data modeling.
  * ETL Tools and Orchestration: Familiarity with ETL tools and frameworks, such as Apache Airflow.
  * Problem-Solving: Strong analytical and problem-solving skills.
  * Collaboration and Communication: Effective communication skills and collaboration within cross-functional teams

## Advantages:

  * Knowledge of data modeling techniques and machine learning model training.
  * Working with online data warehouses such as BigQuery, ClickHouse, Trino (Presto) etc.
  * Prior experience in the geospatial or location analytics domain is plus.

**WHY JOIN PLACER.AI?**

  * Join a rocketship! We are pioneers of a new market that we are creating.
  * Take a central and critical role at Placer.ai.
  * Work with, and learn from, top-notch talent.
  * Competitive salary.
  * Excellent benefits.
  * Fully remote.

**NOTEWORTHY LINKS TO LEARN MORE ABOUT PLACER**

  * Placer.ai in a nutshell
  * Placer.ai's recent $100M round C funding (unicorn valuation!)
  * Placer.ai's data
  * Placer.ai in the news
  * COVID-19 Economic Recovery Dashboard

_Placer.ai is an equal opportunity employer. Placer.aiâ€™s applicants are considered solely based on their qualifications, without regard to an applicantâ€™s disability or need for accommodation. Any Placer.ai applicant who requires reasonable accommodations during the application process should contact Placer.aiâ€™s Human Resources Department to make the need for an accommodation known.._

  
## CLICK TO [APPLY](https://www.remotewlb.com/apply/data-engineer-70021)

